{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276dfc09-9134-4f37-ac7a-5dc570c481c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "import torchmetrics\n",
    "\n",
    "# pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# transformers\n",
    "from transformers import AdamW, ElectraForSequenceClassification, ElectraTokenizer\n",
    "# from transformers import AdamW, XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from transformers.optimization import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50545857-9a1d-481b-935c-8537affe0089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    label_df = pd.read_excel('data/한국표준산업분류(10차)_국문.xlsx', header=2)\n",
    "    label_df = label_df.iloc[:, range(0,5,2)]\n",
    "    label_df = label_df.fillna(method='ffill')\n",
    "    label_df.drop_duplicates(inplace=True)\n",
    "    label_df.reset_index(drop=True, inplace=True)\n",
    "    label_df[['코드.1', '코드.2']] = label_df[['코드.1', '코드.2']].astype(int)\n",
    "    label_df['target'] = label_df[['코드', '코드.1', '코드.2']].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)\n",
    "\n",
    "    df = pd.read_table('data/1. 실습용자료.txt', sep='|', encoding='cp949')\n",
    "    df.fillna('', inplace=True)\n",
    "    df['text'] = df['text_obj'] + ' ' + df['text_mthd'] + ' ' + df['text_deal']\n",
    "    clean = re.compile(\"[^ㄱ-힣 ]\")\n",
    "    df['text'] = df['text'].apply(lambda x: clean.sub(' ', str(x)))\n",
    "    df['target'] = df[['digit_1', 'digit_2', 'digit_3']].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)\n",
    "    \n",
    "    label_dict = {value: idx for idx, value in enumerate(label_df['target'])}\n",
    "    data_list = [[text, label_dict[target]] for text, target in zip(df['text'], df['target'])]\n",
    "    \n",
    "    return label_dict, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931fc649-b282-4c3a-b456-9492b35b035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CONFIG:\n",
    "    LABEL_DICT, DATA_LIST = preprocessing()\n",
    "    CLASSES = len(LABEL_DICT)\n",
    "    MODEL = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=CLASSES)\n",
    "    TOKENIZER = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "    SEED = 3413\n",
    "    BATCH_SIZE = 64\n",
    "    EPOHCS = 30\n",
    "    MAX_LENGTH = 64\n",
    "    LEARNING_RATE = 6e-6\n",
    "    DEVICE = 'cuda'\n",
    "    N_JOBS = 14\n",
    "    FOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43126ac5-1e66-4f67-a8a6-2c3159a4dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitData(pl.LightningDataModule):\n",
    "    def __init__(self, fold, tokenizer, batch_size):\n",
    "        super().__init__()\n",
    "        self.fold = CONFIG.FOLD\n",
    "        self.tokenizer = CONFIG.TOKENIZER\n",
    "        self.batch_size = CONFIG.BATCH_SIZE\n",
    "        self.data_list = CONFIG.DATA_LIST\n",
    "        self.max_length = CONFIG.MAX_LENGTH\n",
    "        self.seed = CONFIG.SEED\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        texts = np.array([i[0] for i in self.data_list])\n",
    "        labels = np.array([i[1] for i in self.data_list])\n",
    "        indices = self.tokenizer.batch_encode_plus(texts,\n",
    "                                                  max_length=self.max_length,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  return_attention_mask=True,\n",
    "                                                  padding='longest',\n",
    "                                                  truncation=True)\n",
    "        \n",
    "        input_ids = np.array(indices['input_ids'])\n",
    "        attention_mask = np.array(indices['attention_mask'])\n",
    "        cross_validation = StratifiedKFold(self.fold, shuffle=True, random_state=self.seed)\n",
    "        \n",
    "#         for fold, (train_idx, val_idx) in enumerate(cross_validation.split(input_ids, labels, attention_mask)):\n",
    "#             train_inputs = input_ids[train_idx]\n",
    "#             train_labels = labels[train_idx]\n",
    "#             train_masks = attention_mask[train_idx]\n",
    "            \n",
    "#             validation_inputs = input_ids[val_idx]\n",
    "#             validation_labels = labels[val_idx]\n",
    "#             validation_masks = attention_mask[val_idx]\n",
    "            \n",
    "        for fold, (train_idx, val_idx) in enumerate(cross_validation.split(input_ids, labels)):\n",
    "            train_inputs = input_ids[train_idx]\n",
    "            train_labels = labels[train_idx]\n",
    "            validation_inputs = input_ids[val_idx]\n",
    "            validation_labels = labels[val_idx]\n",
    "            \n",
    "            if fold == self.fold:\n",
    "                break\n",
    "            \n",
    "        for fold, (train_idx, val_idx) in enumerate(cross_validation.split(attention_mask, labels)):\n",
    "            train_masks = attention_mask[train_idx]\n",
    "            validation_masks = attention_mask[val_idx]\n",
    "            \n",
    "            if fold == self.fold:\n",
    "                break\n",
    "        \n",
    "\n",
    "        self.train_inputs = torch.tensor(train_inputs)\n",
    "        self.validation_inputs = torch.tensor(validation_inputs)\n",
    "        self.train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "        self.validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
    "        self.train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "        self.validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_data = TensorDataset(self.train_inputs, self.train_masks, self.train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        return DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size, num_workers=CONFIG.N_JOBS, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        validation_data = TensorDataset(self.validation_inputs, self.validation_masks, self.validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        return DataLoader(validation_data, sampler=validation_sampler, batch_size=self.batch_size, num_workers=CONFIG.N_JOBS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7759f946-e17a-4548-b550-b08441b151fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CONFIG.MODEL\n",
    "        self.f1_score = torchmetrics.F1Score(num_classes=CONFIG.CLASSES)\n",
    "        \n",
    "    def forward(self, b_input_ids, b_input_mask, b_labels):\n",
    "        output = self.model(b_input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        z = self(b_input_ids, b_input_mask, b_labels)\n",
    "        loss = z[0]\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        z = self(b_input_ids, b_input_mask, b_labels)\n",
    "        val_loss = z[0]\n",
    "        logits = z[1]\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        self.log('val_f1_score', self.f1_score(logits, b_labels), prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(model.parameters(), lr=CONFIG.LEARNING_RATE)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                   num_warmup_steps=0,\n",
    "                                                   num_training_steps=189*CONFIG.EPOHCS)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def flat_accuracy(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940c015-2237-4935-824e-fb51ed5ccad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bigdata/anaconda3/envs/minhk/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "/home/bigdata/anaconda3/envs/minhk/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                             | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model    | ElectraForSequenceClassification | 113 M \n",
      "1 | f1_score | F1Score                          | 0     \n",
      "--------------------------------------------------------------\n",
      "113 M     Trainable params\n",
      "0         Non-trainable params\n",
      "113 M     Total params\n",
      "452.399   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040df0f6686b45219f8b37e2ed881cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3e8477028b40ada60bcf8149f05b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold in range(CONFIG.FOLD):\n",
    "    dm = LitData(fold = fold, tokenizer=CONFIG.TOKENIZER, batch_size = CONFIG.BATCH_SIZE)\n",
    "    chk_callback = ModelCheckpoint(monitor='val_f1_score',\n",
    "                                  filename='model_best',\n",
    "                                  save_top_k=1,\n",
    "                                  mode='max')\n",
    "    \n",
    "    es_callback = EarlyStopping(monitor='val_f1_score',\n",
    "                               min_delta=0.001,\n",
    "                               patience=5,\n",
    "                               verbose=False,\n",
    "                               mode='max')\n",
    "    model = LitModel()\n",
    "    \n",
    "    trainer = pl.Trainer(gpus=1,\n",
    "                        max_epochs=CONFIG.EPOHCS,\n",
    "                        callbacks=[chk_callback, es_callback])\n",
    "    \n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba19583-367b-48d1-9e14-69a05605fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, device, batch_size):\n",
    "    \n",
    "    df = pd.read_table('data/2. 모델개발용자료.txt', sep='|', encoding='cp949')\n",
    "    df.fillna('', inplace=True)\n",
    "    df['text'] = df['text_obj'] + ' ' + df['text_mthd'] + ' ' + df['text_deal']\n",
    "    clean = re.compile(\"[^ㄱ-힣 ]\")\n",
    "    df['text'] = df['text'].apply(lambda x: clean.sub(' ', str(x)))\n",
    "     \n",
    "    texts = df['text'].values\n",
    "\n",
    "    indices = CONFIG.TOKENIZER.batch_encode_plus(texts,\n",
    "                                                 max_length=CONFIG.MAX_LENGTH,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 return_attention_mask=True,\n",
    "                                                 padding='longest',\n",
    "                                                 truncation=True)\n",
    "    \n",
    "    input_ids = indices[\"input_ids\"]\n",
    "    attention_masks = indices[\"attention_mask\"]\n",
    "\n",
    "    test_inputs = torch.tensor(input_ids)\n",
    "    test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    # Create the DataLoader.\n",
    "    test_data = TensorDataset(test_inputs, test_masks)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    print('Predicting labels')\n",
    "    \n",
    "    preds = []\n",
    "    for fold in range(CONFIG.FOLD):\n",
    "        # model.load_state_dict(torch.load(f'./lightning_logs/version_{fold+5}/checkpoints/model_best.ckpt')['state_dict']) ###### version 확인 ######\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        # Predict \n",
    "        for batch in tqdm.notebook.tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, b_input_mask, None)\n",
    "\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "\n",
    "        flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "        preds.append(flat_predictions)\n",
    "        \n",
    "    preds = np.round(np.mean(preds, axis=0), 0)\n",
    "    \n",
    "    label_dict_reverse= dict(map(reversed, CONFIG.LABEL_DICT.items()))\n",
    "    df['label'] = [label_dict_reverse[i] for i in preds]\n",
    "    \n",
    "    df['digit_1'] = df['label'].apply(lambda x: x.split()[0])\n",
    "    df['digit_2'] = df['label'].apply(lambda x: x.split()[1])\n",
    "    df['digit_3'] = df['label'].apply(lambda x: x.split()[2])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560cdbc-7dfa-43be-ba19-d231027f350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = run_inference(model, CONFIG.DEVICE, batch_size=CONFIG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe78a3-5e99-4d6f-aa66-b42e75fb911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.iloc[:, :-2].to_csv('submission_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e7944-1462-4926-a5f4-9434f606815d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
