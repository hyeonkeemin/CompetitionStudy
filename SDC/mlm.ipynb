{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d712b8b1-bb69-4df7-a493-a8452876dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "import torchmetrics\n",
    "\n",
    "# pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "# transformers 모델\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41d31ef-2548-490b-ab32-763312579cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('data/1. 실습용자료.txt', sep='|', encoding='cp949')\n",
    "df.fillna('', inplace=True)\n",
    "df['text'] = df['text_obj'] + ' ' + df['text_mthd'] + ' ' + df['text_deal']\n",
    "# clean = re.compile(\"[^A-Za-zㄱ-힣 ]\")\n",
    "# df['text'] = df['text'].apply(lambda x: clean.sub(' ', str(x)))\n",
    "df['target'] = df[['digit_1', 'digit_2', 'digit_3']].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ac7743-48d3-4cfb-845c-bfdf1cb6ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣a-zA-Z{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64546779-99f2-4b38-a95e-75e81f4744c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = df['text'].map(clean).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef160674-a8ad-4e17-9835-5dd06bb12fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kss import split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c23258-79dd-45ce-9070-e800c129ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./test-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd07c44-0438-4b84-899b-6384d5390a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-02 10:40:08--  https://raw.githubusercontent.com/huggingface/transformers/72aee83ced5f31302c5e331d896412737287f976/examples/pytorch/language-modeling/run_mlm.py\n",
      "raw.githubusercontent.com (raw.githubusercontent.com)을(를) 해석하는 중... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... 접속됨.\n",
      "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 200 OK\n",
      "길이: 24078 (24K) [text/plain]\n",
      "다음 위치에 저장: `run_mlm.py'\n",
      "\n",
      "run_mlm.py          100%[===================>]  23.51K  --.-KB/s    / 0.007s   \n",
      "\n",
      "2022-04-02 10:40:08 (3.47 MB/s) - `run_mlm.py' 저장됨 [24078/24078]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O run_mlm.py https://raw.githubusercontent.com/huggingface/transformers/72aee83ced5f31302c5e331d896412737287f976/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ebf4e4b-9e08-4e03-a162-17b4975420fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a16b3cceeb84951b29a08f4aa8c7237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('korean_petitions_safe.txt', 'w') as f:\n",
    "    for doc in tqdm.notebook.tqdm(contents):\n",
    "        if doc:\n",
    "            f.write(doc+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bac8917-9d1e-4bde-9707-aef8050ca10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카센터에서 자동차부분정비 타이어오일교환\n",
      "상점내에서 일반인을 대상으로 채소.과일판매\n",
      "절단하여사업체에도매 공업용고무를가지고 합성고무도매\n",
      "영업점에서 일반소비자에게 열쇠잠금장치\n",
      "어린이집 보호자의 위탁을 받아 취학전아동보육\n",
      "철 절삭.용접 카프라배관자재\n",
      "음식점에서 접객시설을 갖추고 참치회(일본식)\n",
      "쌀을 가지고 가공하여 떡제조\n",
      "시청에서 재정과인력 일반공공행정 지방행정 집행\n",
      "영업장에서 고객의뢰를 받아 내부전기공사\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 korean_petitions_safe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc192793-0731-4916-ad73-8b5a5d413a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-02 10:59:14.821030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "04/02/2022 10:59:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/02/2022 10:59:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./test-mlm/runs/Apr02_10-59-15_bigdata-System-Product-Name,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./test-mlm,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['mlflow', 'tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./test-mlm,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/02/2022 10:59:16 - WARNING - datasets.builder - Using custom data configuration default-e16c05049e724f8d\n",
      "04/02/2022 10:59:16 - INFO - datasets.builder - Generating dataset text (/home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "Downloading and preparing dataset text/default to /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 3826.92it/s]\n",
      "04/02/2022 10:59:16 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "04/02/2022 10:59:17 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1014.83it/s]\n",
      "04/02/2022 10:59:17 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "04/02/2022 10:59:17 - INFO - datasets.builder - Generating train split\n",
      "04/02/2022 10:59:17 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 587.52it/s]\n",
      "04/02/2022 10:59:18 - WARNING - datasets.builder - Using custom data configuration default-e16c05049e724f8d\n",
      "04/02/2022 10:59:18 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/02/2022 10:59:18 - INFO - datasets.info - Loading Dataset info from /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "04/02/2022 10:59:18 - WARNING - datasets.builder - Reusing dataset text (/home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "04/02/2022 10:59:18 - INFO - datasets.info - Loading Dataset info from /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "04/02/2022 10:59:19 - WARNING - datasets.builder - Using custom data configuration default-e16c05049e724f8d\n",
      "04/02/2022 10:59:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/02/2022 10:59:19 - INFO - datasets.info - Loading Dataset info from /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "04/02/2022 10:59:19 - WARNING - datasets.builder - Reusing dataset text (/home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "04/02/2022 10:59:19 - INFO - datasets.info - Loading Dataset info from /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "[INFO|configuration_utils.py:648] 2022-04-02 10:59:20,323 >> loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/bigdata/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "[INFO|configuration_utils.py:684] 2022-04-02 10:59:20,324 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-04-02 10:59:22,267 >> loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/bigdata/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "[INFO|configuration_utils.py:684] 2022-04-02 10:59:22,269 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-02 10:59:27,206 >> loading file https://huggingface.co/beomi/kcbert-base/resolve/main/vocab.txt from cache at /home/bigdata/.cache/huggingface/transformers/527aa95c387f7c7aa3bebe490a9ede81af16f407b169db730d22632d5822b640.1b39769be8fe13da6152a54d35d7973b687b1aa6067771885d39610963e29dbe\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-02 10:59:27,207 >> loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-02 10:59:27,207 >> loading file https://huggingface.co/beomi/kcbert-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-02 10:59:27,207 >> loading file https://huggingface.co/beomi/kcbert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-02 10:59:27,207 >> loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer_config.json from cache at /home/bigdata/.cache/huggingface/transformers/21078f0099ac15db7a5163f4fea7f742808c30cb0393f1ee56a43dc56d9eb082.cca45b9490565b45e1c62cf5a0529b670fc5ab0db2d4a4af99f6ac577b673eb1\n",
      "[INFO|configuration_utils.py:648] 2022-04-02 10:59:28,006 >> loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/bigdata/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "[INFO|configuration_utils.py:684] 2022-04-02 10:59:28,008 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-04-02 10:59:28,862 >> loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at /home/bigdata/.cache/huggingface/transformers/10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "[INFO|configuration_utils.py:684] 2022-04-02 10:59:28,864 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1431] 2022-04-02 10:59:30,695 >> loading weights file https://huggingface.co/beomi/kcbert-base/resolve/main/pytorch_model.bin from cache at /home/bigdata/.cache/huggingface/transformers/1c204bf1f008ee734eeb5ce678b148d14fa298802ce16d879c92a22a52527a0e.6cdf570ee57a7f6a5c727c436a4c26d8e9601ddaa1377ebcb16b7285d76125cd\n",
      "[WARNING|modeling_utils.py:1693] 2022-04-02 10:59:31,861 >> Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO|modeling_utils.py:1710] 2022-04-02 10:59:31,861 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at beomi/kcbert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Running tokenizer on every text in dataset:   0%|       | 0/950 [00:00<?, ?ba/s]04/02/2022 10:59:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-efa5a3ab52d0c88d.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 950/950 [00:26<00:00, 35.30b\n",
      "Running tokenizer on every text in dataset:   0%|        | 0/50 [00:00<?, ?ba/s]04/02/2022 10:59:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-23668f3723889b94.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 50/50 [00:01<00:00, 35.64ba/\n",
      "Grouping texts in chunks of 300:   0%|                  | 0/950 [00:00<?, ?ba/s]04/02/2022 11:00:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-2303ec9fd19cadef.arrow\n",
      "Grouping texts in chunks of 300: 100%|████████| 950/950 [01:14<00:00, 12.74ba/s]\n",
      "Grouping texts in chunks of 300:   0%|                   | 0/50 [00:00<?, ?ba/s]04/02/2022 11:01:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bigdata/.cache/huggingface/datasets/text/default-e16c05049e724f8d/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-f9c2eb983601cdc9.arrow\n",
      "Grouping texts in chunks of 300: 100%|██████████| 50/50 [00:03<00:00, 13.03ba/s]\n",
      "[INFO|trainer.py:570] 2022-04-02 11:01:23,710 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/bigdata/anaconda3/envs/minhk/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1279] 2022-04-02 11:01:23,720 >> ***** Running training *****\n",
      "[INFO|trainer.py:1280] 2022-04-02 11:01:23,720 >>   Num examples = 41425\n",
      "[INFO|trainer.py:1281] 2022-04-02 11:01:23,720 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1282] 2022-04-02 11:01:23,720 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1283] 2022-04-02 11:01:23,720 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1284] 2022-04-02 11:01:23,720 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1285] 2022-04-02 11:01:23,720 >>   Total optimization steps = 15537\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'loss': 1.836, 'learning_rate': 4.839093776147262e-05, 'epoch': 0.1}           \n",
      "  3%|█▏                                   | 500/15537 [02:09<1:05:02,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:03:34,047 >> Saving model checkpoint to ./test-mlm/checkpoint-500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:03:34,049 >> Configuration saved in ./test-mlm/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:03:34,544 >> Model weights saved in ./test-mlm/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:03:34,545 >> tokenizer config file saved in ./test-mlm/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:03:34,545 >> Special tokens file saved in ./test-mlm/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.4492, 'learning_rate': 4.678187552294523e-05, 'epoch': 0.19}         \n",
      "  6%|██▎                                 | 1000/15537 [04:21<1:02:54,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:05:45,413 >> Saving model checkpoint to ./test-mlm/checkpoint-1000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:05:45,415 >> Configuration saved in ./test-mlm/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:05:45,904 >> Model weights saved in ./test-mlm/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:05:45,905 >> tokenizer config file saved in ./test-mlm/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:05:45,905 >> Special tokens file saved in ./test-mlm/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3251, 'learning_rate': 4.517281328441784e-05, 'epoch': 0.29}         \n",
      " 10%|███▍                                | 1500/15537 [06:32<1:00:44,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:07:56,787 >> Saving model checkpoint to ./test-mlm/checkpoint-1500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:07:56,789 >> Configuration saved in ./test-mlm/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:07:57,296 >> Model weights saved in ./test-mlm/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:07:57,296 >> tokenizer config file saved in ./test-mlm/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:07:57,296 >> Special tokens file saved in ./test-mlm/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.2597, 'learning_rate': 4.356375104589045e-05, 'epoch': 0.39}         \n",
      " 13%|████▉                                 | 2000/15537 [08:44<58:34,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:10:08,172 >> Saving model checkpoint to ./test-mlm/checkpoint-2000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:10:08,173 >> Configuration saved in ./test-mlm/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:10:08,681 >> Model weights saved in ./test-mlm/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:10:08,681 >> tokenizer config file saved in ./test-mlm/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:10:08,681 >> Special tokens file saved in ./test-mlm/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2044, 'learning_rate': 4.195468880736307e-05, 'epoch': 0.48}         \n",
      " 16%|██████                                | 2500/15537 [10:55<56:24,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:12:19,518 >> Saving model checkpoint to ./test-mlm/checkpoint-2500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:12:19,519 >> Configuration saved in ./test-mlm/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:12:19,998 >> Model weights saved in ./test-mlm/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:12:19,999 >> tokenizer config file saved in ./test-mlm/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:12:19,999 >> Special tokens file saved in ./test-mlm/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.169, 'learning_rate': 4.034562656883569e-05, 'epoch': 0.58}          \n",
      " 19%|███████▎                              | 3000/15537 [13:06<54:15,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:14:30,874 >> Saving model checkpoint to ./test-mlm/checkpoint-3000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:14:30,876 >> Configuration saved in ./test-mlm/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:14:31,361 >> Model weights saved in ./test-mlm/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:14:31,362 >> tokenizer config file saved in ./test-mlm/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:14:31,362 >> Special tokens file saved in ./test-mlm/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1408, 'learning_rate': 3.87365643303083e-05, 'epoch': 0.68}          \n",
      " 23%|████████▌                             | 3500/15537 [15:18<52:05,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:16:42,436 >> Saving model checkpoint to ./test-mlm/checkpoint-3500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:16:42,438 >> Configuration saved in ./test-mlm/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:16:42,952 >> Model weights saved in ./test-mlm/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:16:42,953 >> tokenizer config file saved in ./test-mlm/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:16:42,953 >> Special tokens file saved in ./test-mlm/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1254, 'learning_rate': 3.712750209178091e-05, 'epoch': 0.77}         \n",
      " 26%|█████████▊                            | 4000/15537 [17:29<49:55,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:18:53,806 >> Saving model checkpoint to ./test-mlm/checkpoint-4000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:18:53,806 >> Configuration saved in ./test-mlm/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:18:54,282 >> Model weights saved in ./test-mlm/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:18:54,283 >> tokenizer config file saved in ./test-mlm/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:18:54,283 >> Special tokens file saved in ./test-mlm/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0904, 'learning_rate': 3.5518439853253525e-05, 'epoch': 0.87}        \n",
      " 29%|███████████                           | 4500/15537 [19:40<47:46,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:21:05,149 >> Saving model checkpoint to ./test-mlm/checkpoint-4500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:21:05,150 >> Configuration saved in ./test-mlm/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:21:05,638 >> Model weights saved in ./test-mlm/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:21:05,639 >> tokenizer config file saved in ./test-mlm/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:21:05,639 >> Special tokens file saved in ./test-mlm/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.0772, 'learning_rate': 3.390937761472614e-05, 'epoch': 0.97}         \n",
      " 32%|████████████▏                         | 5000/15537 [21:52<45:35,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:23:16,511 >> Saving model checkpoint to ./test-mlm/checkpoint-5000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:23:16,512 >> Configuration saved in ./test-mlm/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:23:17,004 >> Model weights saved in ./test-mlm/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:23:17,005 >> tokenizer config file saved in ./test-mlm/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:23:17,005 >> Special tokens file saved in ./test-mlm/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.0531, 'learning_rate': 3.2300315376198754e-05, 'epoch': 1.06}        \n",
      " 35%|█████████████▍                        | 5500/15537 [24:03<43:26,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:25:27,690 >> Saving model checkpoint to ./test-mlm/checkpoint-5500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:25:27,691 >> Configuration saved in ./test-mlm/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:25:28,203 >> Model weights saved in ./test-mlm/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:25:28,203 >> tokenizer config file saved in ./test-mlm/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:25:28,203 >> Special tokens file saved in ./test-mlm/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0235, 'learning_rate': 3.069125313767137e-05, 'epoch': 1.16}         \n",
      " 39%|██████████████▋                       | 6000/15537 [26:14<41:16,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:27:39,055 >> Saving model checkpoint to ./test-mlm/checkpoint-6000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:27:39,056 >> Configuration saved in ./test-mlm/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:27:39,532 >> Model weights saved in ./test-mlm/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:27:39,533 >> tokenizer config file saved in ./test-mlm/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:27:39,533 >> Special tokens file saved in ./test-mlm/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0122, 'learning_rate': 2.908219089914398e-05, 'epoch': 1.26}         \n",
      " 42%|███████████████▉                      | 6500/15537 [28:26<39:06,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:29:50,378 >> Saving model checkpoint to ./test-mlm/checkpoint-6500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:29:50,378 >> Configuration saved in ./test-mlm/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:29:50,852 >> Model weights saved in ./test-mlm/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:29:50,853 >> tokenizer config file saved in ./test-mlm/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:29:50,853 >> Special tokens file saved in ./test-mlm/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0081, 'learning_rate': 2.7473128660616594e-05, 'epoch': 1.35}        \n",
      " 45%|█████████████████                     | 7000/15537 [30:37<36:56,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:32:01,713 >> Saving model checkpoint to ./test-mlm/checkpoint-7000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:32:01,713 >> Configuration saved in ./test-mlm/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:32:02,189 >> Model weights saved in ./test-mlm/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:32:02,189 >> tokenizer config file saved in ./test-mlm/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:32:02,189 >> Special tokens file saved in ./test-mlm/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.9901, 'learning_rate': 2.5864066422089205e-05, 'epoch': 1.45}        \n",
      " 48%|██████████████████▎                   | 7500/15537 [32:48<34:47,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:34:13,043 >> Saving model checkpoint to ./test-mlm/checkpoint-7500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:34:13,044 >> Configuration saved in ./test-mlm/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:34:13,521 >> Model weights saved in ./test-mlm/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:34:13,521 >> tokenizer config file saved in ./test-mlm/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:34:13,521 >> Special tokens file saved in ./test-mlm/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.9804, 'learning_rate': 2.4255004183561823e-05, 'epoch': 1.54}        \n",
      " 51%|███████████████████▌                  | 8000/15537 [35:00<32:37,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:36:24,376 >> Saving model checkpoint to ./test-mlm/checkpoint-8000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:36:24,377 >> Configuration saved in ./test-mlm/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:36:24,853 >> Model weights saved in ./test-mlm/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:36:24,854 >> tokenizer config file saved in ./test-mlm/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:36:24,854 >> Special tokens file saved in ./test-mlm/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.9641, 'learning_rate': 2.2645941945034434e-05, 'epoch': 1.64}        \n",
      " 55%|████████████████████▊                 | 8500/15537 [37:11<30:26,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:38:35,723 >> Saving model checkpoint to ./test-mlm/checkpoint-8500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:38:35,725 >> Configuration saved in ./test-mlm/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:38:36,237 >> Model weights saved in ./test-mlm/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:38:36,237 >> tokenizer config file saved in ./test-mlm/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:38:36,238 >> Special tokens file saved in ./test-mlm/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.9584, 'learning_rate': 2.103687970650705e-05, 'epoch': 1.74}         \n",
      " 58%|██████████████████████                | 9000/15537 [39:22<28:34,  3.81it/s][INFO|trainer.py:2139] 2022-04-02 11:40:47,089 >> Saving model checkpoint to ./test-mlm/checkpoint-9000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:40:47,090 >> Configuration saved in ./test-mlm/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:40:47,569 >> Model weights saved in ./test-mlm/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:40:47,569 >> tokenizer config file saved in ./test-mlm/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:40:47,569 >> Special tokens file saved in ./test-mlm/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.9557, 'learning_rate': 1.9427817467979663e-05, 'epoch': 1.83}        \n",
      " 61%|███████████████████████▏              | 9500/15537 [41:34<26:07,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:42:58,447 >> Saving model checkpoint to ./test-mlm/checkpoint-9500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:42:58,448 >> Configuration saved in ./test-mlm/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:42:58,938 >> Model weights saved in ./test-mlm/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:42:58,939 >> tokenizer config file saved in ./test-mlm/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:42:58,939 >> Special tokens file saved in ./test-mlm/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.9356, 'learning_rate': 1.7818755229452274e-05, 'epoch': 1.93}        \n",
      " 64%|███████████████████████▊             | 10000/15537 [43:45<23:57,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:45:09,876 >> Saving model checkpoint to ./test-mlm/checkpoint-10000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:45:09,877 >> Configuration saved in ./test-mlm/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:45:10,372 >> Model weights saved in ./test-mlm/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:45:10,372 >> tokenizer config file saved in ./test-mlm/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:45:10,372 >> Special tokens file saved in ./test-mlm/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.9248, 'learning_rate': 1.620969299092489e-05, 'epoch': 2.03}         \n",
      " 68%|█████████████████████████            | 10500/15537 [45:57<21:47,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:47:21,237 >> Saving model checkpoint to ./test-mlm/checkpoint-10500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:47:21,239 >> Configuration saved in ./test-mlm/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:47:21,745 >> Model weights saved in ./test-mlm/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:47:21,746 >> tokenizer config file saved in ./test-mlm/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:47:21,746 >> Special tokens file saved in ./test-mlm/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.9153, 'learning_rate': 1.4600630752397504e-05, 'epoch': 2.12}        \n",
      " 71%|██████████████████████████▏          | 11000/15537 [48:08<19:38,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:49:32,590 >> Saving model checkpoint to ./test-mlm/checkpoint-11000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:49:32,591 >> Configuration saved in ./test-mlm/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:49:33,067 >> Model weights saved in ./test-mlm/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:49:33,067 >> tokenizer config file saved in ./test-mlm/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:49:33,067 >> Special tokens file saved in ./test-mlm/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.9014, 'learning_rate': 1.2991568513870117e-05, 'epoch': 2.22}        \n",
      " 74%|███████████████████████████▍         | 11500/15537 [50:19<17:28,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:51:43,912 >> Saving model checkpoint to ./test-mlm/checkpoint-11500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:51:43,913 >> Configuration saved in ./test-mlm/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:51:44,386 >> Model weights saved in ./test-mlm/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:51:44,387 >> tokenizer config file saved in ./test-mlm/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:51:44,387 >> Special tokens file saved in ./test-mlm/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.8928, 'learning_rate': 1.138250627534273e-05, 'epoch': 2.32}         \n",
      " 77%|████████████████████████████▌        | 12000/15537 [52:31<15:18,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:53:55,311 >> Saving model checkpoint to ./test-mlm/checkpoint-12000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:53:55,313 >> Configuration saved in ./test-mlm/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:53:55,804 >> Model weights saved in ./test-mlm/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:53:55,804 >> tokenizer config file saved in ./test-mlm/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:53:55,804 >> Special tokens file saved in ./test-mlm/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.889, 'learning_rate': 9.773444036815344e-06, 'epoch': 2.41}          \n",
      " 80%|█████████████████████████████▊       | 12500/15537 [54:42<13:08,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:56:06,695 >> Saving model checkpoint to ./test-mlm/checkpoint-12500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:56:06,697 >> Configuration saved in ./test-mlm/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:56:07,187 >> Model weights saved in ./test-mlm/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:56:07,187 >> tokenizer config file saved in ./test-mlm/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:56:07,187 >> Special tokens file saved in ./test-mlm/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.893, 'learning_rate': 8.164381798287959e-06, 'epoch': 2.51}          \n",
      " 84%|██████████████████████████████▉      | 13000/15537 [56:53<10:58,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 11:58:18,055 >> Saving model checkpoint to ./test-mlm/checkpoint-13000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 11:58:18,056 >> Configuration saved in ./test-mlm/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 11:58:18,531 >> Model weights saved in ./test-mlm/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 11:58:18,531 >> tokenizer config file saved in ./test-mlm/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 11:58:18,531 >> Special tokens file saved in ./test-mlm/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.8738, 'learning_rate': 6.555319559760571e-06, 'epoch': 2.61}         \n",
      " 87%|████████████████████████████████▏    | 13500/15537 [59:05<08:49,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 12:00:29,430 >> Saving model checkpoint to ./test-mlm/checkpoint-13500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:00:29,432 >> Configuration saved in ./test-mlm/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:00:29,939 >> Model weights saved in ./test-mlm/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:00:29,939 >> tokenizer config file saved in ./test-mlm/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:00:29,939 >> Special tokens file saved in ./test-mlm/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.8808, 'learning_rate': 4.946257321233186e-06, 'epoch': 2.7}          \n",
      " 90%|███████████████████████████████▌   | 14000/15537 [1:01:16<06:39,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 12:02:40,831 >> Saving model checkpoint to ./test-mlm/checkpoint-14000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:02:40,832 >> Configuration saved in ./test-mlm/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:02:41,306 >> Model weights saved in ./test-mlm/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:02:41,306 >> tokenizer config file saved in ./test-mlm/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:02:41,306 >> Special tokens file saved in ./test-mlm/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.8707, 'learning_rate': 3.3371950827057993e-06, 'epoch': 2.8}         \n",
      " 93%|████████████████████████████████▋  | 14500/15537 [1:03:28<04:29,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 12:04:52,195 >> Saving model checkpoint to ./test-mlm/checkpoint-14500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:04:52,195 >> Configuration saved in ./test-mlm/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:04:52,671 >> Model weights saved in ./test-mlm/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:04:52,671 >> tokenizer config file saved in ./test-mlm/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:04:52,671 >> Special tokens file saved in ./test-mlm/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 0.8764, 'learning_rate': 1.728132844178413e-06, 'epoch': 2.9}          \n",
      " 97%|█████████████████████████████████▊ | 15000/15537 [1:05:39<02:19,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 12:07:03,549 >> Saving model checkpoint to ./test-mlm/checkpoint-15000\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:07:03,550 >> Configuration saved in ./test-mlm/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:07:04,025 >> Model weights saved in ./test-mlm/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:07:04,025 >> tokenizer config file saved in ./test-mlm/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:07:04,025 >> Special tokens file saved in ./test-mlm/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 0.8744, 'learning_rate': 1.1907060565102659e-07, 'epoch': 2.99}        \n",
      "100%|██████████████████████████████████▉| 15500/15537 [1:07:50<00:09,  3.85it/s][INFO|trainer.py:2139] 2022-04-02 12:09:14,967 >> Saving model checkpoint to ./test-mlm/checkpoint-15500\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:09:14,968 >> Configuration saved in ./test-mlm/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:09:15,460 >> Model weights saved in ./test-mlm/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:09:15,460 >> tokenizer config file saved in ./test-mlm/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:09:15,461 >> Special tokens file saved in ./test-mlm/checkpoint-15500/special_tokens_map.json\n",
      "100%|██████████████████████████████████▉| 15536/15537 [1:08:01<00:00,  3.85it/s][INFO|trainer.py:1508] 2022-04-02 12:09:25,891 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 4082.1732, 'train_samples_per_second': 30.443, 'train_steps_per_second': 3.806, 'train_loss': 1.0431137697541852, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████| 15537/15537 [1:08:01<00:00,  3.81it/s]\n",
      "[INFO|trainer.py:2139] 2022-04-02 12:09:26,087 >> Saving model checkpoint to ./test-mlm\n",
      "[INFO|configuration_utils.py:439] 2022-04-02 12:09:26,089 >> Configuration saved in ./test-mlm/config.json\n",
      "[INFO|modeling_utils.py:1084] 2022-04-02 12:09:26,600 >> Model weights saved in ./test-mlm/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2094] 2022-04-02 12:09:26,601 >> tokenizer config file saved in ./test-mlm/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2100] 2022-04-02 12:09:26,601 >> Special tokens file saved in ./test-mlm/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     1.0431\n",
      "  train_runtime            = 1:08:02.17\n",
      "  train_samples            =      41425\n",
      "  train_samples_per_second =     30.443\n",
      "  train_steps_per_second   =      3.806\n"
     ]
    }
   ],
   "source": [
    "!python run_mlm.py \\\n",
    "    --model_name_or_path beomi/kcbert-base \\\n",
    "    --train_file korean_petitions_safe.txt \\\n",
    "    --do_train \\\n",
    "    --output_dir ./test-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a4816-78bf-4465-9754-c434ea156445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
